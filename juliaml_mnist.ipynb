{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Note: you may have to add/clone/checkout some of these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this re-exports Transformations, StochasticOptimization, Penalties, and ObjectiveFunctions\n",
    "using Learn\n",
    "\n",
    "# my version of ML iteration.  Hopefully will be replaced with what's currently in MLDataUtils dev branch\n",
    "using StochasticOptimization.Iteration\n",
    "\n",
    "import MLDataUtils: rescale!\n",
    "\n",
    "# for loading the data\n",
    "import MNIST\n",
    "\n",
    "# for plotting\n",
    "using StatPlots, MLPlots\n",
    "gr(leg=false, linealpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_test_loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a one-hot matrix given class labels\n",
    "# TODO: this should be added as a utility in MLDataUtils\n",
    "function to_one_hot(y::AbstractVector)\n",
    "    yint = map(yi->round(Int,yi)+1, y)\n",
    "    nclasses = maximum(yint)\n",
    "    hot = zeros(Float64, nclasses, length(y))\n",
    "    for (i,yi) in enumerate(yint)\n",
    "        hot[yi,i] = 1.0\n",
    "    end\n",
    "    hot\n",
    "end\n",
    "\n",
    "# randomly pick a subset of testdata (size = totcount) and compute the total loss\n",
    "function my_test_loss(obj, testdata, totcount = 500)\n",
    "    totloss = 0.0\n",
    "    totcorrect = 0\n",
    "    for (x,y) in each_obs(rand(each_obs(testdata), totcount))\n",
    "        totloss += transform!(obj,y,x)\n",
    "\n",
    "        # logistic version:\n",
    "        # ŷ = output_value(obj.transformation)[1]\n",
    "        # correct = (ŷ > 0.5 && y > 0.5) || (ŷ <= 0.5 && y < 0.5)\n",
    "\n",
    "        # softmax version:\n",
    "        ŷ = output_value(obj.transformation)\n",
    "        chosen_idx = indmax(ŷ)\n",
    "        correct = y[chosen_idx] > 0\n",
    "\n",
    "        totcorrect += correct\n",
    "    end\n",
    "    totloss, totcorrect/totcount\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our data:\n",
    "x_train, y_train = MNIST.traindata()\n",
    "x_test, y_test = MNIST.testdata()\n",
    "\n",
    "# normalize the input data given μ/σ for the input training data\n",
    "# note: scale both train and test sets using the train data\n",
    "μ, σ = rescale!(x_train)\n",
    "rescale!(x_test, μ, σ)\n",
    "\n",
    "# convert y data to one-hot\n",
    "y_train, y_test = map(to_one_hot, (y_train, y_test))\n",
    "\n",
    "# optional: limit to only 0/1 digits for easier training\n",
    "# to_isone(y::AbstractVector) = (z = Array(eltype(y), 1, length(y)); map!(yi->float(yi==1.0), z, y))\n",
    "# y_train, y_test = map(to_isone, (y_train, y_test))\n",
    "# train = filterobs(i -> y_train[i] < 1.5, x_train, y_train)\n",
    "# test = filterobs(i -> y_test[i] < 1.5, x_test, y_test)\n",
    "\n",
    "# store as tuples to make it easier\n",
    "train = (x_train, y_train)\n",
    "test = (x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct our model and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectiveFunctions.RegularizedObjective{Transformations.Chain{Float64,Transformations.Params{SubArray{Float64,1,Array{Float64,1},Tuple{UnitRange{Int64}},true},Tuple{},Tuple{}}},ObjectiveFunctions.CrossEntropy{Float64},Penalties.L2Penalty{Float64}}(Chain{Float64}(\n",
       "   Affine{784-->100}\n",
       "   softplus{100}\n",
       "   Affine{100-->100}\n",
       "   softplus{100}\n",
       "   Affine{100-->10}\n",
       "   softmax{10}\n",
       ") ,ObjectiveFunctions.CrossEntropy{Float64}(10,Transformations.InputNode{:+,Float64,1}(Transformations.Node[Transformations.OutputNode{Float64,1}(Transformations.Node[Transformations.InputNode{:+,Float64,1}(#= circular reference @-4 =#)],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.InputNode{:+,Float64,1}(Transformations.Node[],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],Dict{Transformations.OutputNode{Float64,1},Int64}()),Transformations.OutputNode{Float64,1}(Transformations.Node[],[0.0],[1.0])),Penalties.L2Penalty{Float64}(1.0e-5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nin, nh, nout = 784, [100,100], 10\n",
    "\n",
    "# create a feedforward neural net with softplus activations and softmax output\n",
    "t = nnet(nin, nout, nh, :softplus, :softmax)\n",
    "\n",
    "# create an objective function with L2 penalty and an implicit cross entropy loss layer\n",
    "penalty = L2Penalty(1e-5)\n",
    "obj = objective(t, penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional: set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "(totloss,accuracy) = (553.208203251883,0.085)\n"
     ]
    }
   ],
   "source": [
    "# the parts of the plot\n",
    "chainplt = ChainPlot(t, maxn=100)\n",
    "lossplt = TracePlot(title=\"Test Loss\", ylim=(0,Inf))\n",
    "accuracyplt = TracePlot(title=\"Accuracy\", ylim=(0.6,1))\n",
    "hmplt = heatmap(rand(28,28), ratio=1)\n",
    "\n",
    "# put together the full plot... a ChainPlot with loss, accuracy, and the heatmap\n",
    "plot(\n",
    "    chainplt.plt,\n",
    "    lossplt.plt,\n",
    "    accuracyplt.plt,\n",
    "    hmplt,\n",
    "    size = (1200,800),\n",
    "    layout=@layout([a; grid(1,3){0.2h}])\n",
    ")\n",
    "\n",
    "doanim = false\n",
    "# anim = Animation()\n",
    "\n",
    "# this is our custom callback which will be called on every 100 iterations\n",
    "# note: we do the plotting here.\n",
    "tracer = IterFunction((obj, i) -> begin\n",
    "    # sample points from the test set and compute/save the loss\n",
    "    @show i\n",
    "    if mod1(i,500)==500\n",
    "        totloss, accuracy = my_test_loss(obj, test, 200)\n",
    "        @show totloss, accuracy\n",
    "        push!(lossplt, i, totloss)\n",
    "        push!(accuracyplt, i, accuracy)\n",
    "    end\n",
    "\n",
    "    # add transformation data\n",
    "    update!(chainplt)\n",
    "\n",
    "    # update the heatmap of the total outgoing weight from each pixel\n",
    "    pixel_importance = reshape(sum(t[1].params.views[1],1), 28, 28)\n",
    "    # pixel_importance = reshape(abs(input_grad(t)),28,28)  # another possible metric\n",
    "    hmplt[1][1][:z].surf[:] = pixel_importance\n",
    "\n",
    "    # handle animation frames/output\n",
    "    if doanim\n",
    "        lastframe = 5000\n",
    "        if i < lastframe\n",
    "            frame(anim)\n",
    "        elseif i == lastframe\n",
    "            gif(anim, fps=10)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # display the plot\n",
    "    gui()\n",
    "end, every=100)\n",
    "\n",
    "# trace once before we start learning to see initial values\n",
    "tracer.f(obj, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MetaLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StochasticOptimization.MetaLearner{Tuple{StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.Adam{Float64},StochasticOptimization.GradientAverager},StochasticOptimization.IterFunction,StochasticOptimization.MaxIter}}((StochasticOptimization.GradientLearner{StochasticOptimization.FixedLR,StochasticOptimization.Adam{Float64},StochasticOptimization.GradientAverager}(StochasticOptimization.FixedLR(0.001),StochasticOptimization.Adam{Float64}(1.0e-8,0.9,0.999,#undef,#undef,#undef,#undef),StochasticOptimization.GradientAverager(#undef)),StochasticOptimization.IterFunction(#3,100),StochasticOptimization.MaxIter(10000)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = make_learner(\n",
    "    # averages the gradient over minibatches, updating params using the Adam method\n",
    "    GradientLearner(1e-3, Adam()),\n",
    "\n",
    "    # our custom iteration method\n",
    "    tracer,\n",
    "\n",
    "    # shorthand to add a MaxIter(10000)\n",
    "    maxiter = 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 100\n",
      "i = 200\n",
      "i = 300\n",
      "i = 400\n",
      "i = 500\n",
      "(totloss,accuracy) = (61.21660254169188,0.885)\n",
      "i = 600\n",
      "i = 700\n",
      "i = 800\n",
      "i = 900\n",
      "i = 1000\n",
      "(totloss,accuracy) = (45.61347885553403,0.94)\n",
      "i = 1100\n",
      "i = 1200\n",
      "i = 1300\n",
      "i = 1400\n",
      "i = 1500\n",
      "(totloss,accuracy) = (67.33544983904031,0.91)\n",
      "i = 1600\n",
      "i = 1700\n",
      "i = 1800\n",
      "i = 1900\n",
      "i = 2000\n",
      "(totloss,accuracy) = (50.41961108075616,0.93)\n",
      "i = 2100\n",
      "i = 2200\n",
      "i = 2300\n",
      "i = 2400\n",
      "i = 2500\n",
      "(totloss,accuracy) = (41.661604603977565,0.93)\n",
      "i = 2600\n",
      "i = 2700\n",
      "i = 2800\n",
      "i = 2900\n",
      "i = 3000\n",
      "(totloss,accuracy) = (45.66251742528191,0.92)\n",
      "i = 3100\n",
      "i = 3200\n",
      "i = 3300\n",
      "i = 3400\n",
      "i = 3500\n",
      "(totloss,accuracy) = (65.63228608079748,0.905)\n",
      "i = 3600\n",
      "i = 3700\n",
      "i = 3800\n",
      "i = 3900\n",
      "i = 4000\n",
      "(totloss,accuracy) = (34.78429839828591,0.94)\n",
      "i = 4100\n",
      "i = 4200\n",
      "i = 4300\n",
      "i = 4400\n",
      "i = 4500\n",
      "(totloss,accuracy) = (49.777691320751586,0.955)\n",
      "i = 4600\n",
      "i = 4700\n",
      "i = 4800\n",
      "i = 4900\n",
      "i = 5000\n",
      "(totloss,accuracy) = (48.28631176577039,0.945)\n",
      "i = 5100\n",
      "i = 5200\n",
      "i = 5300\n",
      "i = 5400\n",
      "i = 5500\n",
      "(totloss,accuracy) = (92.717103022676,0.945)\n",
      "i = 5600\n",
      "i = 5700\n",
      "i = 5800\n",
      "i = 5900\n",
      "i = 6000\n",
      "(totloss,accuracy) = (37.00207296419798,0.96)\n",
      "i = 6100\n",
      "i = 6200\n",
      "i = 6300\n",
      "i = 6400\n",
      "i = 6500\n",
      "(totloss,accuracy) = (43.95941554528182,0.925)\n",
      "i = 6600\n",
      "i = 6700\n",
      "i = 6800\n",
      "i = 6900\n",
      "i = 7000\n",
      "(totloss,accuracy) = (27.672211941026625,0.97)\n",
      "i = 7100\n",
      "i = 7200\n",
      "i = 7300\n",
      "i = 7400\n",
      "i = 7500\n",
      "(totloss,accuracy) = (79.72357282694863,0.93)\n",
      "i = 7600\n",
      "i = 7700\n",
      "i = 7800\n",
      "i = 7900\n",
      "i = 8000\n",
      "(totloss,accuracy) = (41.04465731632583,0.95)\n",
      "i = 8100\n",
      "i = 8200\n",
      "i = 8300\n",
      "i = 8400\n",
      "i = 8500\n",
      "(totloss,accuracy) = (20.859009936365638,0.965)\n",
      "i = 8600\n",
      "i = 8700\n",
      "i = 8800\n",
      "i = 8900\n",
      "i = 9000\n",
      "(totloss,accuracy) = (24.614986512050994,0.945)\n",
      "i = 9100\n",
      "i = 9200\n",
      "i = 9300\n",
      "i = 9400\n",
      "i = 9500\n",
      "(totloss,accuracy) = (25.075340703245605,0.945)\n",
      "i = 9600\n",
      "i = 9700\n",
      "i = 9800\n",
      "i = 9900\n",
      "i = 10000\n",
      "(totloss,accuracy) = (33.92256176750176,0.955)\n"
     ]
    }
   ],
   "source": [
    "# do the learning... average over minibatches of size 5 for maxiter iterations\n",
    "learn!(obj, learner, infinite_batches(train, size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save an image of the training output\n",
    "png(\"/tmp/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc4",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
