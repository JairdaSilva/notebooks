{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Note: you may have to add/clone/checkout some of these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this re-exports Transformations, StochasticOptimization, Penalties, and ObjectiveFunctions\n",
    "using Learn\n",
    "\n",
    "# my version of ML iteration.  Hopefully will be replaced with what's currently in MLDataUtils dev branch\n",
    "using StochasticOptimization.Iteration\n",
    "\n",
    "import MLDataUtils: rescale!\n",
    "\n",
    "# for loading the data\n",
    "import MNIST\n",
    "\n",
    "# for plotting\n",
    "using StatPlots, MLPlots\n",
    "gr(leg=false, linealpha=0.5, legendfont=font(7), fmt=:png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a one-hot matrix given class labels\n",
    "# TODO: this should be added as a utility in MLDataUtils\n",
    "function to_one_hot(y::AbstractVector)\n",
    "    yint = map(yi->round(Int,yi)+1, y)\n",
    "    nclasses = maximum(yint)\n",
    "    hot = zeros(Float64, nclasses, length(y))\n",
    "    for (i,yi) in enumerate(yint)\n",
    "        hot[yi,i] = 1.0\n",
    "    end\n",
    "    hot\n",
    "end\n",
    "\n",
    "# randomly pick a subset of testdata (size = totcount) and compute the total loss\n",
    "function my_test_loss(obj, testdata, totcount = 500)\n",
    "    totloss = 0.0\n",
    "    totcorrect = 0\n",
    "    for (x,y) in each_obs(rand(each_obs(testdata), totcount))\n",
    "        totloss += transform!(obj,y,x)\n",
    "\n",
    "        # logistic version:\n",
    "        # ŷ = output_value(obj.transformation)[1]\n",
    "        # correct = (ŷ > 0.5 && y > 0.5) || (ŷ <= 0.5 && y < 0.5)\n",
    "\n",
    "        # softmax version:\n",
    "        ŷ = output_value(obj.transformation)\n",
    "        chosen_idx = indmax(ŷ)\n",
    "        correct = y[chosen_idx] > 0\n",
    "\n",
    "        totcorrect += correct\n",
    "    end\n",
    "    totloss, totcorrect/totcount\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our data:\n",
    "x_train, y_train = MNIST.traindata()\n",
    "x_test, y_test = MNIST.testdata()\n",
    "\n",
    "# normalize the input data given μ/σ for the input training data\n",
    "# note: scale both train and test sets using the train data\n",
    "μ, σ = rescale!(x_train)\n",
    "rescale!(x_test, μ, σ)\n",
    "# xmin, xmax = extrema(x_train)\n",
    "# x_train .= 2 .* (x_train .- xmin) ./ (xmax - xmin) .- 1\n",
    "# x_test .= 2 .* (x_test .- xmin) ./ (xmax - xmin) .- 1\n",
    "\n",
    "# convert y data to one-hot\n",
    "y_train, y_test = map(to_one_hot, (y_train, y_test))\n",
    "\n",
    "# optional: limit to only 0/1 digits for easier training\n",
    "# to_isone(y::AbstractVector) = (z = Array(eltype(y), 1, length(y)); map!(yi->float(yi==1.0), z, y))\n",
    "# y_train, y_test = map(to_isone, (y_train, y_test))\n",
    "# train = filterobs(i -> y_train[i] < 1.5, x_train, y_train)\n",
    "# test = filterobs(i -> y_test[i] < 1.5, x_test, y_test)\n",
    "\n",
    "# store as tuples to make it easier\n",
    "train = (x_train, y_train)\n",
    "test = (x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct our model and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nin, nh, nout = 784, [50,50,50], 10\n",
    "\n",
    "# this is our gradient calculation method\n",
    "grad_calc = :backprop\n",
    "# grad_calc = :dfa\n",
    "\n",
    "# create a feedforward neural net with softplus activations and softmax output\n",
    "t = nnet(nin, nout, nh, :sign, :softmax, grad_calc=grad_calc)\n",
    "\n",
    "# create an objective function with L2 penalty and an implicit cross entropy loss layer\n",
    "penalty = L2Penalty(1e-2)\n",
    "obj = objective(t, penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optional: set up plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the parts of the plot\n",
    "chainplt = ChainPlot(t, maxn=50, tickfont=font(5))\n",
    "lossplt = TracePlot(2, title=\"Loss\", ylim=(0,Inf), leg=true, lab=[\"Train\" \"Test\"])\n",
    "accuracyplt = TracePlot(2, title=\"Accuracy\", ylim=(0.4,1), leg=true, lab=[\"Train\" \"Test\"])\n",
    "hmplt = heatmap(rand(28,28), ratio=1, title=\"outgoing wgt\")\n",
    "hmplt2 = heatmap(rand(28,28), ratio=1, title=\"input grad\")\n",
    "\n",
    "# put together the full plot... a ChainPlot with loss, accuracy, and the heatmap\n",
    "plot(\n",
    "    chainplt.plt,\n",
    "    lossplt.plt,\n",
    "    accuracyplt.plt,\n",
    "    hmplt,\n",
    "    hmplt2,\n",
    "    size = (1200,700),\n",
    "    layout=@layout([a{0.8h}; grid(1,2){0.75w} grid(1,2)])\n",
    ")\n",
    "\n",
    "# anim = nothing\n",
    "anim = Animation()\n",
    "\n",
    "# this is our custom callback which will be called on every 100 iterations\n",
    "# note: we do the plotting here.\n",
    "tracer = IterFunction((obj, i) -> begin\n",
    "    # sample points from the test set and compute/save the loss\n",
    "#     @show i\n",
    "    if mod1(i,500)==500\n",
    "        # training loss\n",
    "        trainloss, trainaccuracy = my_test_loss(obj, train, 200)\n",
    "#         @show trainloss, trainaccuracy\n",
    "        \n",
    "        testloss, testaccuracy = my_test_loss(obj, test, 200)\n",
    "#         @show testloss, testaccuracy\n",
    "        \n",
    "        push!(lossplt, i, [trainloss, testloss])\n",
    "        push!(accuracyplt, i, [trainaccuracy, testaccuracy])\n",
    "    end\n",
    "\n",
    "    # add transformation data\n",
    "    update!(chainplt)\n",
    "\n",
    "    # update the heatmap of the total outgoing weight from each pixel\n",
    "    t1 = isa(t[1], InputNorm) ? t[2] : t[1]\n",
    "    pixel_importance = reshape(sum(t1.params.views[1],1), 28, 28)\n",
    "    hmplt[1][1][:z].surf[:] = pixel_importance\n",
    "    \n",
    "    pixel_importance = reshape(abs(input_grad(t)),28,28)  # another possible metric\n",
    "    hmplt2[1][1][:z].surf[:] = pixel_importance\n",
    "\n",
    "    # handle animation frames/output\n",
    "    anim == nothing || frame(anim)\n",
    "\n",
    "    # update the plot display\n",
    "#     gui()\n",
    "    inline()\n",
    "end, every=250)\n",
    "\n",
    "# trace once before we start learning to see initial values\n",
    "tracer.f(obj, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MetaLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learner = make_learner(\n",
    "    # averages the gradient over minibatches, updating params using the Adam method\n",
    "    GradientLearner(1e-3, SGD(0.7)),\n",
    "\n",
    "    # our custom iteration method\n",
    "    tracer,\n",
    "\n",
    "    # shorthand to add a MaxIter(10000)\n",
    "    maxiter = 30000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do the learning... average over minibatches of size 5 for maxiter iterations\n",
    "# learn!(obj, learner, infinite_batches(train, size=5))\n",
    "learn!(obj, learner, infinite_obs(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save an image of the training output\n",
    "png(\"/tmp/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the animation\n",
    "mp4(anim, fps=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.1-pre",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
