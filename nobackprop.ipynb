{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "gr(leg=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the function we're trying to minimize...\n",
    "# note: the 2-arg version is ONLY used to plot the surface\n",
    "f(x,y) = sin(x)+cos(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a surface with a close-up view\n",
    "x = linspace(2,6,100)\n",
    "surface(x,x,f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a surface with a zoomed-out view\n",
    "x = linspace(2,40,200)\n",
    "surface(x,x,f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this reexports Transformations and StochasticOptimization, which is what\n",
    "# we'll use to minimize our function f\n",
    "using Learn\n",
    "\n",
    "# define the function to minimize... x is assumed to be a length-2 vector\n",
    "f(x) = sin(x[1])+cos(x[2])\n",
    "\n",
    "# define the derivative of the function to minimize\n",
    "df(x) = [cos(x[1]), -sin(x[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is ONLY for the plotting... set up an animation, add a new series\n",
    "# to track the learning curve, and define the method that will get called\n",
    "# once per iteration in the call to learn!\n",
    "anim = Animation()\n",
    "p = path3d!(1, l=(:black,2), m=(:circle,5))\n",
    "function addpt(m,i)\n",
    "    θ = params(m)\n",
    "    push!(p, 2, θ..., f(θ))\n",
    "    mod1(i,20)==1 && frame(anim)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !! this is important !!\n",
    "# tfunc is a convenience constructor to build a Transformations.Differentiable\n",
    "# In this case we are building a OnceDifferentiable, similar to what's available in Optim.\n",
    "# We pass in an arbitrary function, the size of the params θ, and a function that returns\n",
    "# a deriv/gradient at θ.  Since we didn't specify a size for inputs, it is assumed 0.\n",
    "# So this is a \"closed system\"... f is not a function of inputs x, only of params θ.\n",
    "# We are finding a minimum of a function by moving the params θ closer to the argmin of f.\n",
    "t = tfunc(f, 2, df)\n",
    "\n",
    "# I wanted to define a very specific initial point θ₀ so the animation looked nice\n",
    "params(t)[:] = [2,5]\n",
    "\n",
    "# At this point we have a transformation t <: OnceDifferentiable, and we're going to call\n",
    "#   learn!(t, metalearner)\n",
    "# We build a generic MetaLearner using the `make_learner` convenience.  The constructor accepts zero\n",
    "# or more LearningStrategy objects, plus there are a few keywords (just for convenience!!) to add\n",
    "# common strategies.\n",
    "learn!(t, make_learner(\n",
    "\n",
    "    # This is a strategy that updates the parameters of the transformation `t` using the Adam method.\n",
    "    # The learning rate is fixed at 1e-2\n",
    "    GradientLearner(1e-2,Adam()),\n",
    "\n",
    "    # This is a strategy that evaluates the function, and compares the result to the last result.\n",
    "    # When the difference is sufficiently close, we are done.\n",
    "    Converged(m -> params(m)),\n",
    "\n",
    "    # This is a convenience to add a `MaxIter` strategy, which stops us after 1000 iterations\n",
    "    maxiter = 1000,\n",
    "\n",
    "    # This is a convenience to add a `IterFunction` strategy, which calls a function every iteration:\n",
    "    #   f(model, itr_num)\n",
    "    # In this case: model == t\n",
    "    oniter = addpt\n",
    "))\n",
    "\n",
    "# NOTE: there are many more strategies and settings... until I fill out the documentation, look at the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output the animation\n",
    "gif(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore this... old code\n",
    "x,y = Plots.unzip(pts)\n",
    "plot!(x,y,map(f,x,y),l=(:black,2),m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc4",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
